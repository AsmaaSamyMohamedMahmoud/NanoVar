
Here is an example to illustrate the evaluation process during SV benchmarking:

We have a ground-truth of a single deletion SV in chr1:10000-15000. The deleted region is from 10000 to 15000 and it consist of two breakends (10000 and 15000).

A ground-truth SV breakend region BED file is created with 400 bp error:
chr1	9600	10400	Region1
chr1	14600	15400	Region2

Here are examples of predicted SVs from three tools and their evaluation without considering SV class annotation:

Tool A:
Predicted deletion: chr1:10005-14998 
Breakend1: 10005 (Falls within Region1)
Breakend2: 14998 (Falls within Region2)
Recall: 2 out of 2 (100%)
Precision: 2 out of 2 (100%)

Tool B:
Predicted deletion: chr1:9950-10500
Breakend1: 9950 (Falls within Region1)
Breakend2: 10500 (Falls within none)
Recall: 1 out of 2 (50%)
Precision: 1 out of 2 (50%)

Tool C:
Predicted deletion: chr1: 5000-6000
Breakend1: 5000 (Falls within none)
Breakend2: 6000 (Falls within none)
Recall: 0 out of 2 (0%)
Precision: 0 out of 2 (0%)


If SV class annotation is to be considered (e.g. Radar charts - Figure 2c and 2d), the evaluation will have to be done on concordant SV class hits between predicted and actual (refer to sv_class_radar.txt for details).